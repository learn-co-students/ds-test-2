{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Module 4 Assessment</h1>\n",
    "\n",
    "## Overview\n",
    "\n",
    "This assessment is designed to test your understanding of the Mod 4 material. It covers:\n",
    "\n",
    "* Calculus, Cost Function, and Gradient Descent\n",
    "* Extensions to Linear Models\n",
    "* Introduction to Linear Regression\n",
    "* Time Series Modeling\n",
    "\n",
    "\n",
    "Read the instructions carefully. You will be asked both to write code and respond to a few short answer questions.\n",
    "\n",
    "### Note on the short answer questions\n",
    "\n",
    "For the short answer questions please use your own words. The expectation is that you have not copied and pasted from an external source, even if you consult another source to help craft your response. While the short answer questions are not necessarily being assessed on grammatical correctness or sentence structure, do your best to communicate yourself clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Calculus, Cost Function, and Gradient Descent\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![best fit line](visuals/best_fit_line.png)\n",
    "\n",
    "The best fit line that goes through the scatterplot up above can be generalized in the following equation: $$y = mx + b$$\n",
    "\n",
    "Of all the possible lines, we can prove why that particular line was chosen using the plot down below:\n",
    "\n",
    "![](visuals/cost_curve.png)\n",
    "\n",
    "where RSS is defined as the residual sum of squares:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "RSS &= \\sum_{i=1}^n(actual - expected)^2 \\\\\n",
    "&= \\sum_{i=1}^n(y_i - \\hat{y})^2 \\\\\n",
    "&= \\sum_{i=1}^n(y_i - (mx_i + b))^2\n",
    "\\end{align}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is a more generalized name for the RSS curve above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual sum of squares curve above is a specific example of a cost curve. In machine learning models, the goal is to minimize the cost curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2A. Would you rather choose a $m$ value of 0.08 or 0.03 from the curve up above? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be better to have a value of 0.03 rather than 0.08 in the cost curve above. The reason for this is that the RSS is lower for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2B. Explain what it means to move along the curve in relation to the best fit line with respect to the $m$ value you chose in 2A. What is the slope of the above cost curve when it has reached its minimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As m changes values from 0.00 to 0.10 the Residual Sum of Squares is changing. The higher the value of the RSS, the worse the model is performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](visuals/gd.png)\n",
    "\n",
    "### 3. Using the gradient descent visual from above, explain why the distance between steps is getting smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance between the steps is getting smaller because the slope gradually becomes less and less steep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is the purpose of a learning rate in gradient descent? Explain how a very small and a very large learning rate would affect the gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate is a number ranging from 0.0 to 1.0 that is multiplied by each step that is taken during gradient descent. If the learning rate is smaller, the step sizes will become smaller. If the learning rate is larger, the step sizes will be larger, up until the point where the learning rate is 1.0, and it is the same as moving along the gradient normally. Learning rate is present in gradient descent to help ensure that an optimal minimum on the cost curve is discovered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Extensions to Linear Regression\n",
    "---\n",
    "\n",
    "In this section, you're going to be creating linear models that are more complicated than a simple linear regression. In the cells below, we are importing relevant modules that you might need later on. We also load and prepare the dataset for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error, roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>147.042500</td>\n",
       "      <td>23.264000</td>\n",
       "      <td>30.554000</td>\n",
       "      <td>14.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>85.854236</td>\n",
       "      <td>14.846809</td>\n",
       "      <td>21.778621</td>\n",
       "      <td>5.217457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>74.375000</td>\n",
       "      <td>9.975000</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>10.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>149.750000</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>12.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>218.825000</td>\n",
       "      <td>36.525000</td>\n",
       "      <td>45.100000</td>\n",
       "      <td>17.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>296.400000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TV       radio   newspaper       sales\n",
       "count  200.000000  200.000000  200.000000  200.000000\n",
       "mean   147.042500   23.264000   30.554000   14.022500\n",
       "std     85.854236   14.846809   21.778621    5.217457\n",
       "min      0.700000    0.000000    0.300000    1.600000\n",
       "25%     74.375000    9.975000   12.750000   10.375000\n",
       "50%    149.750000   22.900000   25.750000   12.900000\n",
       "75%    218.825000   36.525000   45.100000   17.400000\n",
       "max    296.400000   49.600000  114.000000   27.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('raw_data/advertising.csv').drop('Unnamed: 0',axis=1)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('sales', axis=1)\n",
    "y = data['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing set\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y,random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. We'd like to add a bit of complexity to the model created in the example above, and we will do it by adding some polynomial terms. Write a Function to calculate train and test error for different polynomial degree (1-9).\n",
    "\n",
    "This function should:\n",
    "* take `poly_degree` as a parameter that will be used to create all different possible polynomial degrees starting at 1 UP TO and including poly_degree\n",
    "* as you create the PolynomialFeatures object and fit linear regression models\n",
    "* calculate the root mean square error for each level of polynomial\n",
    "* return two lists that contain the `train_errors` and `test_errors` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_degree(poly_degree):\n",
    "    '''calculate train and test error for different polynomial degree (1-9)'''\n",
    "    train_errors = None\n",
    "    test_errors = None\n",
    "    \n",
    "#       For each model with different polynomial degree:\n",
    "#       Create object for PolynomialFeatures with necessary parameters\n",
    "#       get the polynomialfeatures for training and testing\n",
    "#       Fit the LinearRegression to new Features and y_train\n",
    "#       Calculate the testing and training error (Root mean squared error)\n",
    "#       Add train and test errors to numpy arrays\n",
    "        \n",
    "    return train_errors, test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def calc_degree(poly_degree):\n",
    "    \"\"\"Calculate train and test error for different polynomial degree (1-9)\"\"\"\n",
    "    train_error = np.zeros(len(y_train))\n",
    "    test_error = np.zeros(len(y_test))\n",
    "    for i in range(1, poly_degree + 1):\n",
    "        poly = PolynomialFeatures(degree=i, interaction_only=False, include_bias=False)\n",
    "        X_poly_train = poly.fit_transform(X_train)\n",
    "        X_poly_test = poly.transform(X_test)\n",
    "        \n",
    "        lr_poly = LinearRegression()\n",
    "        lr_poly.fit(X_poly_train,y_train)\n",
    "\n",
    "        cv_train = np.sqrt(mean_squared_error(y_train, lr_poly.predict(X_poly_train)))\n",
    "        cv_test = np.sqrt(mean_squared_error(y_test, lr_poly.predict(X_poly_test)))\n",
    "\n",
    "        train_error[i-1] = cv_train\n",
    "        test_error[i-1]  = cv_test\n",
    "\n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def calc_degree_2(poly_degree):\n",
    "    \"\"\"Calculate train and test error for different polynomial degree (1-9)\"\"\"\n",
    "    train_error = []\n",
    "    test_error = []\n",
    "    for i in range(1, poly_degree + 1):\n",
    "        poly = PolynomialFeatures(degree=i, interaction_only=False)\n",
    "        X_poly_train = poly.fit_transform(X_train)\n",
    "        X_poly_test = poly.transform(X_test)\n",
    "        lr_poly = LinearRegression()\n",
    "        lr_poly.fit(X_poly_train,y_train)\n",
    "\n",
    "        cv_train = np.sqrt(mean_squared_error(y_train, lr_poly.predict(X_poly_train)))\n",
    "        cv_test = np.sqrt(mean_squared_error(y_test, lr_poly.predict(X_poly_test)))\n",
    "#         print(cv_train)\n",
    "        train_error.append(cv_train)\n",
    "        test_error.append(cv_test)\n",
    "\n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train = calc_degree(10)[0].tolist()\n",
    "error_test = calc_degree(10)[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5.16147132339435,\n",
       "  1.633049529710119,\n",
       "  0.6544219763525787,\n",
       "  0.4923003895833528,\n",
       "  0.42636966692892925,\n",
       "  0.2552375092236587,\n",
       "  0.21455738787043777,\n",
       "  0.17677574592197967,\n",
       "  0.20526596216126342,\n",
       "  0.26914830727034605,\n",
       "  0.28892220322372025],\n",
       " [5.343670690119708,\n",
       "  1.8399932733741966,\n",
       "  0.4317931087085349,\n",
       "  0.39091400558118194,\n",
       "  1.3972328447228304,\n",
       "  2.381671115675543,\n",
       "  4.672887984282909,\n",
       "  5.391079429485139,\n",
       "  88.12110401687424,\n",
       "  24002.511402029148,\n",
       "  177660.21087344288])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_degree_2(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#error_train = [1.6872219375656985, 0.6412867668294241, 0.4711637753473785, 0.36944896135164984, \n",
    "                0.2410561613114944, 0.2564919445411836, 0.3165437580993593,\n",
    "                0.3903584583915307, 0.32526008157451436, 0.7955206228171482]\n",
    "#error_test = [1.6340815294866986, 0.4951157689460942, 0.4798020916835737, 0.4870596445796682,\n",
    "               0.4608682794261582, 1.198082221301244, 4.440474842230711, \n",
    "               19.919529327003232, 129.4605767258836, 1066.1109145234598]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How does increasing polynomial degree effect Bias and Variance? \n",
    "\n",
    "![rsme](visuals/rsme_poly.png)\n",
    "\n",
    "<!---\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "degree = list(range(1, 10 + 1))\n",
    "ax.plot(degree, error_train[0:len(degree)], \"-\", label=\"Train Error\")\n",
    "ax.plot(degree, error_test[0:len(degree)], \"-\", label=\"Test Error\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Polynomial Feature Degree\")\n",
    "ax.set_ylabel(\"Root Mean Squared Error\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Relationship Between Degree and Error\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"visuals/rsme_poly.png\",\n",
    "            dpi=150,\n",
    "            bbox_inches=\"tight\")\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the polynomial features, it is going to cause our training error to decrease, which decreases the bias but increases the variance (the testing error increases). This is an example of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. In general what are methods would you can use to mitigate overfitting and underfitting? Provide an example for both and explain how both of them work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting: Regularization. With regularization, more complex models are penalized. This ensures that the models are not trained to too much \"noise.\"\n",
    "\n",
    "Underfitting: Feature engineering. By adding additional features, you enable your machine learning models to gain insights about your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is the alpha in Lasso Regression? Find the optimal alpha value for Lasso Regression for the given polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "poly = PolynomialFeatures(degree=10, interaction_only=False, include_bias=False)\n",
    "X_poly_train = poly.fit_transform(X_train) \n",
    "X_poly_test = poly.transform(X_test)\n",
    "pickle.dump(X_poly_train, open(\"write_data/poly_train_model.pkl\", \"wb\"))\n",
    "pickle.dump(X_poly_test, open(\"write_data/poly_test_model.pkl\", \"wb\"))\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly_train = pickle.load(open(\"write_data/poly_train_model.pkl\", \"rb\"))\n",
    "X_poly_test = pickle.load(open(\"write_data/poly_test_model.pkl\", \"rb\"))\n",
    "size = 100\n",
    "alphas = np.linspace(0, 1, size)\n",
    "\n",
    "error = np.zeros(size)\n",
    "for i, alpha in enumerate(np.linspace(.1, 10, size)):\n",
    "    # for each alpha value between .1 and 10\n",
    "    # create a Lasso model\n",
    "    lasso = Lasso(alpha=alpha, tol=0.06)\n",
    "    # fit X_poly_train and y_train onto the Lasso model \n",
    "    lasso.fit(X_poly_train,y_train)\n",
    "    # calculate the predictions from the Lasso model\n",
    "    y_pred = lasso.predict(X_poly_test)\n",
    "    # calculate the MSE from your predictions\n",
    "    score = mean_squared_error(y_pred, y_test)\n",
    "    # place the MSE inside of the error object\n",
    "    error[i] = score\n",
    "\n",
    "alphas[np.argmin(error)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction to Logistic Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "# load data\n",
    "ads_df = pd.read_csv(\"raw_data/social_network_ads.csv\")\n",
    "\n",
    "# one hot encode categorical feature\n",
    "def is_female(x):\n",
    "    \"\"\"Returns 1 if Female; else 0\"\"\"\n",
    "    if x == \"Female\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "ads_df[\"Female\"] = ads_df[\"Gender\"].apply(is_female)\n",
    "ads_df.drop([\"User ID\", \"Gender\"], axis=1, inplace=True)\n",
    "ads_df.head()\n",
    "\n",
    "# separate features and target\n",
    "X = ads_df.drop(\"Purchased\", axis=1)\n",
    "y = ads_df[\"Purchased\"]\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=19)\n",
    "\n",
    "# preprocessing\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train = scale.transform(X_train)\n",
    "X_test = scale.transform(X_test)\n",
    "\n",
    "# save preprocessed train/test split objects\n",
    "pickle.dump(X_train, open(\"write_data/social_network_ads/X_train_scaled.pkl\", \"wb\"))\n",
    "pickle.dump(X_test, open(\"write_data/social_network_ads/X_test_scaled.pkl\", \"wb\"))\n",
    "pickle.dump(y_train, open(\"write_data/social_network_ads/y_train.pkl\", \"wb\"))\n",
    "pickle.dump(y_test, open(\"write_data/social_network_ads/y_test.pkl\", \"wb\"))\n",
    "\n",
    "# build model\n",
    "model = LogisticRegression(C=1e5, solver=\"lbfgs\")\n",
    "model.fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# create confusion matrix\n",
    "# tn, fp, fn, tp\n",
    "cnf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "cnf_matrix\n",
    "\n",
    "# build confusion matrix plot\n",
    "plt.imshow(cnf_matrix,  cmap=plt.cm.Blues) #Create the basic matrix.\n",
    "\n",
    "# Add title and Axis Labels\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Add appropriate Axis Scales\n",
    "class_names = set(y_test) #Get class labels to add to matrix\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# Add Labels to Each Cell\n",
    "thresh = cnf_matrix.max() / 2. #Used for text coloring below\n",
    "#Here we iterate through the confusion matrix and append labels to our visualization.\n",
    "for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "        plt.text(j, i, cnf_matrix[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cnf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "# Add a Side Bar Legend Showing Colors\n",
    "plt.colorbar()\n",
    "\n",
    "# Add padding\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"visuals/cnf_matrix.png\",\n",
    "            dpi=150,\n",
    "            bbox_inches=\"tight\")\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cnf matrix](visuals/cnf_matrix.png)\n",
    "\n",
    "### 1. Using the confusion matrix up above, calculate precision, recall, and F-1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// your answer here //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explain how precision is different from recall and why you should consider using the F-1 score when you are evaulating your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// your answer here //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "# save preprocessed train/test split objects\n",
    "X_train = pickle.load(open(\"write_data/social_network_ads/X_train_scaled.pkl\", \"rb\"))\n",
    "X_test = pickle.load(open(\"write_data/social_network_ads/X_test_scaled.pkl\", \"rb\"))\n",
    "y_train = pickle.load(open(\"write_data/social_network_ads/y_train.pkl\", \"rb\"))\n",
    "y_test = pickle.load(open(\"write_data/social_network_ads/y_test.pkl\", \"rb\"))\n",
    "\n",
    "# build model\n",
    "model = LogisticRegression(C=1e5, solver=\"lbfgs\")\n",
    "model.fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "labels = [\"Age\", \"Estimated Salary\", \"Female\", \"All Features\"]\n",
    "colors = sns.color_palette(\"Set2\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "# add one ROC curve per feature\n",
    "for feature in range(3):\n",
    "    # female feature is one hot encoded so it produces an ROC point rather than a curve\n",
    "    # for this reason, female will not be included in the plot at all since it is\n",
    "    # disingeneuous to call it a curve.\n",
    "    if feature == 2:\n",
    "        pass\n",
    "    else:\n",
    "        X_train_feat = X_train[:, feature].reshape(-1, 1)\n",
    "        X_test_feat = X_test[:, feature].reshape(-1, 1)\n",
    "        logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='lbfgs')\n",
    "        model_log = logreg.fit(X_train_feat, y_train)\n",
    "        y_score = model_log.decision_function(X_test_feat)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "        lw = 2\n",
    "        plt.plot(fpr, tpr, color=colors[feature],\n",
    "                 lw=lw, label=labels[feature])\n",
    "\n",
    "# add one ROC curve with all the features\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "y_score = model_log.decision_function(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color=colors[3], lw=lw, label=labels[3])\n",
    "\n",
    "# create foundation of the plot\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i / 20.0 for i in range(21)])\n",
    "plt.xticks([i / 20.0 for i in range(21)])\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"visuals/many_roc.png\",\n",
    "            dpi=150,\n",
    "            bbox_inches=\"tight\")\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pick the best ROC curve and explain your choice.\n",
    "\n",
    "*Note: each ROC curve represents one model, each labeled with the feature(s) inside each model*.\n",
    "\n",
    "![many roc](visuals/many_roc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// your answer here //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "# sorting by 'Purchased' and then dropping the last 130 records\n",
    "dropped_df = ads_df.sort_values(by=\"Purchased\")[:-130]\n",
    "dropped_df.reset_index(inplace=True)\n",
    "pickle.dump(dropped_df, open(\"write_data/sample_network_data.pkl\", \"wb\"))\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df = pickle.load(open(\"write_data/sample_network_data.pkl\", \"rb\"))\n",
    "\n",
    "# partion features and target \n",
    "X = network_df.drop(\"Purchased\", axis=1)\n",
    "y = network_df[\"Purchased\"]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2019)\n",
    "\n",
    "# scale features\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train = scale.transform(X_train)\n",
    "X_test = scale.transform(X_test)\n",
    "\n",
    "# build classifier\n",
    "model = LogisticRegression(C=1e5, solver=\"lbfgs\")\n",
    "model.fit(X_train,y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# get the accuracy score\n",
    "print(f\"The original classifier has an accuracy score of {round(accuracy_score(y_test, y_test_pred), 3)}.\")\n",
    "\n",
    "# get the area under the curve from an ROC curve\n",
    "y_score = model.decision_function(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "auc = round(roc_auc_score(y_test, y_score), 3)\n",
    "print(f\"The original classifier has an area under the ROC curve of {auc}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The model above has an accuracy score that it too good to believe. Using `y.value_counts()`, explain how `y` is affecting the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// your answer here //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Update the inputs in the classification model using a technique to address the issues mentioned up above in question 4. \n",
    "\n",
    "Be sure to include updates regarding:\n",
    "* the accuracy score; and\n",
    "* the area under the curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=2019)\n",
    "X_train_resampled, y_train_resampled = smote.fit_sample(X_train, y_train) \n",
    "model_smote = LogisticRegression(C=1e5, solver=\"lbfgs\")\n",
    "model_smote.fit(X_train_resampled, y_train_resampled)\n",
    "y_test_pred_smote = model_smote.predict(X_test)\n",
    "y_train_pred_smote = model_smote.predict(X_train_resampled)\n",
    "\n",
    "# assess accuracy\n",
    "score_smote = round(accuracy_score(y_test, y_test_pred_smote), 3)\n",
    "print(f\"The updated classifier has an accuracy score of {score_smote}.\")\n",
    "\n",
    "y_score_smote = model_smote.decision_function(X_test)\n",
    "fpr_smote, tpr_smote, _ = roc_curve(y_test, y_score_smote)\n",
    "auc_smote = roc_auc_score(y_test, y_score_smote)\n",
    "print(f\"The updated classifier has an area under the ROC curve of {auc_smote}.\")\n",
    "\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr,\n",
    "         lw=lw, label=\"Class imbalanced\")\n",
    "plt.plot(fpr_smote, tpr_smote,\n",
    "         lw=lw, label=\"Class balanced after using SMOTE\")\n",
    "# create foundation of the plot\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i / 20.0 for i in range(21)])\n",
    "plt.xticks([i / 20.0 for i in range(21)])\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
